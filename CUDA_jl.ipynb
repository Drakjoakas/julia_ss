{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA.jl\n",
    "\n",
    "La biblioteca `CUDA.jl` te permite programar GPU's de NVIDIA usando el lenguaje de programación Julia. Este paquete permite la programación a varios **niveles de abstracción**, desde el uso de _arreglos_, hasta la escritura de _kernels_ usando la API de CUDA en un nivel más bajo.\n",
    "\n",
    "## Primeros Pasos\n",
    "\n",
    "Lo primero es instalar el paquete usando `Pkg.add(\"CUDA\")`. Lo único que necesita es teer instalado un controlador de NVIDIA. El toolkit de CUDA se descargará automáicamente al usar por primera vez la librería. Posteriormente, solo queda utilizarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La primer gran herramienta con la que cuenta `CUDA.jl` es el tipo de dato `CuArray`. Este tipo de dato es igual a un arreglo normal con la peculiaridad de que se almacena en la memoria del GPU, así como que las operaciones realizadas con éstos se harán de manera automática en el GPU. \n",
    "\n",
    "Esta implementación es de tan alto nivel que no es necesario utilizar los arreglos dentro de un kernel específico para el GPU, si no que puedes utilizarlo tal y como viene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1048576-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " ⋮\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N = 2^20\n",
    "x_d = CUDA.fill(1.0f0, N)\n",
    "y_d = CUDA.fill(2.0f0, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo, para sumar ambos arreglos, podemos usar las características del Julia, en este caso, **broadcasting**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_d .+= x_d\n",
    "\n",
    "@test all(Array(y_d) .== 3.0f0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escribiendo un Kernel\n",
    "\n",
    "Aunque la implementación de alto nivel del uso de arreglos de CUDA fue bastante sencilla, seguimos sin saber qué es lo que está pasando. Si quisiéramos hacer un primer intento para implementar la misma operación en un kernel, haríamos lo siguiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function gpu_add1!(y, x)\n",
    "    for i = 1:length(y)\n",
    "        @inbounds y[i] += x[i]\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "fill!(y_d, 2)\n",
    "\n",
    "@cuda gpu_add1!(y_d,x_d)\n",
    "@test all(Array(y_d) .== 3.0f0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algunas cosas ha resaltar en el programa anterior\n",
    "\n",
    "Un **kernel** es una función que se ejecuta dentro del GPU. Para programar un kernel en Julia, se escribe una función común. Posteriormente hablaremos sobre API's que puedes usar dentro del kernel para el manejo de datos dentro del GPU. \n",
    "\n",
    "Algo importante a la hora de escribir un kernel es que la función **no debe regresar nada**. Es por eso que esta función al final tiene la sentencia `return nothing`, aunque podría ser simplemente un `return`.\n",
    "\n",
    "Otra cosa a destacar es la forma en que invocamos a la función. Para hacerlo, es necesario utilizar la macro `@cuda` y llamar a la función. Dentro de esta macro se agregan los parámetros de la llamada al kernel, que veremos más adelante.\n",
    "\n",
    "Sin embargo, si comparamos los tiempos de ejecución del kernel que escribimos con la implementación en alto nivel, veremos que algo está pasando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 10000 samples with 1 evaluation.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m66.903 μs\u001b[22m\u001b[39m … \u001b[35m185.503 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m68.445 μs               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m68.919 μs\u001b[22m\u001b[39m ± \u001b[32m  2.487 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m0.00% ± 0.00%\n",
       "\n",
       "  \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\u001b[39m▃\u001b[39m▅\u001b[39m▇\u001b[39m█\u001b[39m▇\u001b[34m▆\u001b[39m\u001b[39m▄\u001b[39m▃\u001b[32m▁\u001b[39m\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \n",
       "  \u001b[39m▁\u001b[39m▁\u001b[39m▂\u001b[39m▃\u001b[39m▆\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[34m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[32m█\u001b[39m\u001b[39m▇\u001b[39m▅\u001b[39m▅\u001b[39m▄\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▂\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m \u001b[39m▃\n",
       "  66.9 μs\u001b[90m         Histogram: frequency by time\u001b[39m         75.3 μs \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m592 bytes\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m11\u001b[39m."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function add_broadcast!(y,x)\n",
    "    CUDA.@sync y .+= x #para sincronizar la ejecución de la función y que se espere a que termine\n",
    "    return\n",
    "end\n",
    "\n",
    "@benchmark add_broadcast!($y_d, $x_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 91 samples with 1 evaluation.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m54.790 ms\u001b[22m\u001b[39m … \u001b[35m55.083 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m54.957 ms              \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m54.951 ms\u001b[22m\u001b[39m ± \u001b[32m41.173 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m0.00% ± 0.00%\n",
       "\n",
       "  \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▂\u001b[39m \u001b[39m▁\u001b[39m \u001b[32m▂\u001b[39m\u001b[39m█\u001b[34m▅\u001b[39m\u001b[39m▁\u001b[39m▇\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \n",
       "  \u001b[39m▃\u001b[39m▁\u001b[39m▁\u001b[39m▃\u001b[39m▁\u001b[39m▃\u001b[39m▁\u001b[39m▃\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▅\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▃\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▅\u001b[39m▃\u001b[39m▁\u001b[39m▃\u001b[39m▅\u001b[39m▅\u001b[39m█\u001b[39m▆\u001b[39m█\u001b[39m█\u001b[32m█\u001b[39m\u001b[39m█\u001b[34m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▅\u001b[39m▃\u001b[39m▁\u001b[39m▃\u001b[39m▃\u001b[39m▅\u001b[39m▃\u001b[39m▅\u001b[39m▃\u001b[39m \u001b[39m▁\n",
       "  54.8 ms\u001b[90m         Histogram: frequency by time\u001b[39m          55 ms \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m3.25 KiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m52\u001b[39m."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function bench_gpu1!(y,x)\n",
    "    CUDA.@sync begin\n",
    "        @cuda gpu_add1!(y,x)\n",
    "    end\n",
    "end\n",
    "\n",
    "@benchmark bench_gpu1!($y_d,$x_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar, el tiempo de ejecución de nuestro kernel es **significativamente** más tardado que la implementación en alto nivel. Esto es debido a que no estamos aprovechando las capacidades de ejecución en paralelo del GPU. En este caso, nuestro kernel se está ejecutando a un solo hilo dentro del GPU (que es considerablemente más lento que la ejecución serial en CPU). Es por eso que es necesario escribir un kernel **en paralelo**.\n",
    "\n",
    "## Escribiendo un kernel en paralelo\n",
    "\n",
    "Para paralelizarlo, es necesario asignar diferentes tareas a diferentes hilos. Para facilitar esto, cada hilo en el GPU tiene acceso a diferentes variables que los identifican de manera única, por ejemplo, su **id de hilo**.\n",
    "\n",
    "Los hilos de CUDA cuentan con las variables `threadIdx` y `blockDim` que representan el índice del hilo y el número de bloques en ejecución (véase conceptos base de CUDA). Cada variable tiene 3 campos `x`, `y` y `z`. Podemos utilizar lo anterior para escribir el siguiente kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function gpu_add2!(y,x)\n",
    "    index  = threadIdx().x\n",
    "    stride = blockDim().x\n",
    "    for i = index:stride:length(y)\n",
    "        @inbounds y[i] += x[i]\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "fill!(y_d, 2)\n",
    "@cuda threads=256 gpu_add2!(y_d,x_d)\n",
    "@test all(Array(y_d) .== 3.0f0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nótese que al invocar el kernel, agregamos la opción `threads=256`, lo que divide el trabajo entre 256 hilos de manera lineal. Ahora veamos su tiempo de ejecución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 2803 samples with 1 evaluation.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m1.632 ms\u001b[22m\u001b[39m … \u001b[35m  6.370 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m1.744 ms               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m1.767 ms\u001b[22m\u001b[39m ± \u001b[32m112.153 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m0.00% ± 0.00%\n",
       "\n",
       "  \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▂\u001b[39m▂\u001b[39m▃\u001b[39m▅\u001b[39m▅\u001b[39m█\u001b[39m▇\u001b[39m▇\u001b[39m█\u001b[39m▄\u001b[39m▆\u001b[34m▄\u001b[39m\u001b[39m▂\u001b[39m \u001b[39m \u001b[32m \u001b[39m\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\u001b[39m▅\u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \n",
       "  \u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▂\u001b[39m▂\u001b[39m▄\u001b[39m▅\u001b[39m▇\u001b[39m▇\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[34m█\u001b[39m\u001b[39m█\u001b[39m▇\u001b[39m▆\u001b[32m▅\u001b[39m\u001b[39m▄\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▄\u001b[39m▇\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m▆\u001b[39m▇\u001b[39m▇\u001b[39m▄\u001b[39m▄\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▂\u001b[39m▂\u001b[39m▃\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m \u001b[39m▄\n",
       "  1.63 ms\u001b[90m         Histogram: frequency by time\u001b[39m        1.97 ms \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m3.11 KiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m49\u001b[39m."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function bench_gpu2!(y,x)\n",
    "    CUDA.@sync begin\n",
    "        @cuda threads=256 gpu_add2!(y,x)\n",
    "    end\n",
    "end\n",
    "\n",
    "@benchmark bench_gpu2!($y_d, $x_d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aun que ya mejoró bastante el tiempo de ejecución, aún seguimos por debajo de la implementación de alto nivel. Para alcanzarlo, es necesario paralelizar un poco más.\n",
    "\n",
    "Las GPU tienen un número limitado de hilos que pueden correr en un solo _streaming multiprocessor_ (SM), pero también tienen varios SM. Para utilizarlos todos, es necesario correr el kernel en múltiples _bloques_. Dividiremos el trabajo de la siguiente manera:\n",
    "\n",
    "\n",
    "<img src=\"https://cuda.juliagpu.org/stable/tutorials/intro1.png\" width=1000>\n",
    "\n",
    "El diagrama muestra la numeración de los bloques en la biblioteca para C/C++. En Julia, los bloques comienzan con 1 en lugar de 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function gpu_add3!(y,x)\n",
    "    index  = (blockIdx().x -1) * blockDim().x + threadIdx().x\n",
    "    stride = gridDim().x * blockDim().x \n",
    "\n",
    "    for i = index:stride:length(y)\n",
    "        @inbounds y[i] += x[i]\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "numblocks = ceil(Int, N/256)\n",
    "\n",
    "fill!(y_d,2)\n",
    "@cuda threads=256 blocks=numblocks gpu_add3!(y_d,x_d)\n",
    "@test all(Array(y_d) .== 3.0f0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 10000 samples with 1 evaluation.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m62.592 μs\u001b[22m\u001b[39m … \u001b[35m170.801 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m63.868 μs               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m64.062 μs\u001b[22m\u001b[39m ± \u001b[32m  1.771 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m0.00% ± 0.00%\n",
       "\n",
       "  \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\u001b[39m▃\u001b[39m▄\u001b[39m▅\u001b[39m▄\u001b[39m▅\u001b[39m▅\u001b[39m▆\u001b[34m█\u001b[39m\u001b[32m▇\u001b[39m\u001b[39m▆\u001b[39m▄\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \n",
       "  \u001b[39m▁\u001b[39m▂\u001b[39m▃\u001b[39m▆\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[34m█\u001b[39m\u001b[32m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▆\u001b[39m▄\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▂\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m \u001b[39m▃\n",
       "  62.6 μs\u001b[90m         Histogram: frequency by time\u001b[39m         69.2 μs \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m336 bytes\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m5\u001b[39m."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function bench_gpu3!(y,x)\n",
    "    numblocks = ceil(Int, length(y)/256)\n",
    "    CUDA.@sync begin\n",
    "        @cuda threads=256 blocks=numblocks gpu_add3!(y,x)\n",
    "    end\n",
    "end\n",
    "\n",
    "@benchmark bench_gpu3!($y_d, $x_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar, el tiempo de ejecución mejoró aun más, alcanzando la implementación de alto nivel. Con estos elementos ya podrías construir tus propios kernels y ejecutarlos con el número de hilos y bloques que desees, así como usar todo el potencial de los `CuArray`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lenguaje de Kernel\n",
    "\n",
    "Vamos a revisar algunas funcionalidades del paquete `CUDA.jl` que permiten escribir kernels todavía más poderosos, acercándose al poder de CUDA.c\n",
    "\n",
    "### Índices y Dimensiones\n",
    "\n",
    "- `gridDim()`: Regresa la dimensiones de la malla.\n",
    "\n",
    "- `blockIdx()`: Regresa el índice del bloque dentro de la malla.\n",
    "- `blockDim()`: Regresa las dimensiones del bloque.\n",
    "- `threadIdx()`: Regresa el índice del hilo dentro del bloque.\n",
    "- `warpsize()`: Regresa el tamaño del warp (conjunto de hilos).\n",
    "- `laneid()`: Regresa el carril del hilo dentro del warp.\n",
    "\n",
    "## Tipos de Memorias\n",
    "\n",
    "### Memoria Compartida\n",
    "\n",
    "- `CuStaticSharedArray(T::Type, dims)`: Devuelve un arreglo de tipo `T` y dimensiones `dims` que apunta a un pedazo de memoria compartida que se asignó de manera estática. El tipo se debe inferir de manera estática y las dimensiones deben ser constantes; de lo contrario, marcará un error.\n",
    "\n",
    "- `CuDynamicSharedArray(T::Type, dims, offset::Integer=0)`: Devuelve un arreglo de tipo `T`y dimensiones `dims` que apuntan a un pedazo de memoria compartida asignada de manera dinámica. El tipo debe de inferirse de manera estática, de lo contrario marcará un error. La cantidad de memoria que se asignará de manera dinámica debe de especificarse al lanzar el kernel.\n",
    "\n",
    "\n",
    "                @cuda threads=n_threads blocks=n_blocks shmem=memSize kernel(d_out, d_in)\n",
    "\n",
    "### Memoria Texture\n",
    "\n",
    "- `CuTexture{T,N,P}`: Memoria de tipo texture `N`-dimensional de tipo `T`. Estos objetos no almacenan memoria por sí solas, si no que se pasan como parámetro al kernel, donde interactuará con el tipo `CuDeviceTexture`. (**Atención: API experimental suejta a cambios**).\n",
    "\n",
    "## Sincronización\n",
    "\n",
    "- `CUDA.sync_threads()`: Espera a que todos los hilos dentro del bloque hayan llegado al punto, y todos los accesos a memoria global realizados antes de la llamada sean visibles para todos los hilos.\n",
    "\n",
    "- `CUDA.sync_threads_and(predicado::{Int32, Boolean})`: Similar a `sync_threads()` pero permite evaluar un predicado para todos los hilos dentro del bloque.\n",
    "\n",
    "Para más información acerca del paquete `CUDA.jl`, puedes revisar la (documentación oficial)[https://cuda.juliagpu.org/stable/] de la biblioteca.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.2",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
