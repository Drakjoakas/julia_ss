{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Abstractions\n",
    "\n",
    "El paquete `KernelAbstractions.jl` te permite escribir kernels similares a un kernel de GPU que se puede ejecutar en distintos dispositivos. Su intención es ser una biblioteca mínima con buen rendimiento que permita escribir código sencillo y heterogéneo. Es una alternativa a utilizar el paquete `CUDA.jl` que además permite ejecutar los mismos kernels en CPU y en GPU.\n",
    "\n",
    "El uso de kernels para GPU proviene del hecho de que hoy en día, el uso de tarjetas gráficas para cómputo científico ha demostrado ser bastante eficiente.\n",
    "\n",
    "## ¿Cómo programar un GPU?\n",
    "En general se hace debe tener esto en cuenta:\n",
    "* Modelo SPMD (_Single Process Multiple Data_)\n",
    "* Se programa con una perspectiva de **camino** (llamado hilo por Nvidia)\n",
    "* Un grupo de caminos se ejecutan de manera conjunta (warp/vector)\n",
    "* La ejecución se organiza en bloques/grupos que pueden acceder a recursos compartidos\n",
    "* La GPU se basan en el rendimiento\n",
    "* Se debe tener en cuenta la arquitectura de la memoria y el uso de ancho de banda al diseñar un kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "using KernelAbstractions, CUDAKernels\n",
    "using CUDA\n",
    "using Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para escribir un kernel usando `KernelAbstractions.jl` es necesario declarar una función usando el macro `@kernel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mul2 (generic function with 5 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@kernel function mul2(A)\n",
    "    I = @index(Global)\n",
    "    A[I] = 2 * A[I]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para lanzar el kernel es necesario invocar la función con el **primer parámetro** siendo el **dispositivo donde ejecutar el kernel**. El **segundo argumento** es el **tamaño del grupo de trabajo**, y el **tercero** es un rango `ndrange` estático. El segundo y tercer argumento son opcionales.\n",
    "\n",
    "Después de instanciar el kernel lo puedes lanzar llamando el kernel con los argumentos correctos y palabras clave para configurar el lanzamiento del kernel. El lanzamiento de los kernels es asíncrono, por lo que para ver los resultados es necesario _capturar el evento_ y _esperar a que termine_.\n",
    "\n",
    "El `ndrange` se parte en bloques, cada bloque es ejecutado por **un grupo de trabajo**. Por ejemplo, funciona bien para:\n",
    "* Grupo de trabajo = (4, 4)\n",
    "* ndrange = (16, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32×32 Matrix{Float64}:\n",
       " 2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0  …  2.0  2.0  2.0  2.0  2.0  2.0  2.0\n",
       " 2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0     2.0  2.0  2.0  2.0  2.0  2.0  2.0\n",
       " 2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0     2.0  2.0  2.0  2.0  2.0  2.0  2.0\n",
       " 2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0     2.0  2.0  2.0  2.0  2.0  2.0  2.0\n",
       " 2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0     2.0  2.0  2.0  2.0  2.0  2.0  2.0\n",
       " 2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0  …  2.0  2.0  2.0  2.0  2.0  2.0  2.0\n",
       " 2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0     2.0  2.0  2.0  2.0  2.0  2.0  2.0\n",
       " 2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0     2.0  2.0  2.0  2.0  2.0  2.0  2.0\n",
       " 2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0     2.0  2.0  2.0  2.0  2.0  2.0  2.0\n",
       " 2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0     2.0  2.0  2.0  2.0  2.0  2.0  2.0\n",
       " ⋮                        ⋮              ⋱  ⋮                        ⋮    \n",
       " 2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0     2.0  2.0  2.0  2.0  2.0  2.0  2.0\n",
       " 2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0     2.0  2.0  2.0  2.0  2.0  2.0  2.0\n",
       " 2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0  …  2.0  2.0  2.0  2.0  2.0  2.0  2.0\n",
       " 2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0     2.0  2.0  2.0  2.0  2.0  2.0  2.0\n",
       " 2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0     2.0  2.0  2.0  2.0  2.0  2.0  2.0\n",
       " 2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0     2.0  2.0  2.0  2.0  2.0  2.0  2.0\n",
       " 2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0     2.0  2.0  2.0  2.0  2.0  2.0  2.0\n",
       " 2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0  …  2.0  2.0  2.0  2.0  2.0  2.0  2.0\n",
       " 2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0     2.0  2.0  2.0  2.0  2.0  2.0  2.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A = ones(32,32)\n",
    "kernel = mul2(CPU(),16)\n",
    "event = kernel(A,ndrange=size(A))\n",
    "wait(event)\n",
    "@show A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para lanzar el mismo kernel con la GPU es necesario indicar `CUDADevice()` como dispositivo, así como utilizar el tipo de datos adecuado como argumento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Excessive output truncated after 4195339 bytes."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B = "
     ]
    },
    {
     "data": {
      "text/plain": [
       "1024×1024 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:\n",
       " 2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0  …  2.0  2.0  2.0  2.0  2.0  2.0  2.0\n",
       " 2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0     2.0  2.0  2.0  2.0  2.0  2.0  2.0\n",
       " 2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0     2.0  2.0  2.0  2.0  2.0  2.0  2.0\n",
       " 2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0     2.0  2.0  2.0  2.0  2.0  2.0  2.0\n",
       " 2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0     2.0  2.0  2.0  2.0  2.0  2.0  2.0\n",
       " 2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0  …  2.0  2.0  2.0  2.0  2.0  2.0  2.0\n",
       " 2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0     2.0  2.0  2.0  2.0  2.0  2.0  2.0\n",
       " 2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0     2.0  2.0  2.0  2.0  2.0  2.0  2.0\n",
       " 2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0     2.0  2.0  2.0  2.0  2.0  2.0  2.0\n",
       " 2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0     2.0  2.0  2.0  2.0  2.0  2.0  2.0\n",
       " ⋮                        ⋮              ⋱                 ⋮              \n",
       " 2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0  …  2.0  2.0  2.0  2.0  2.0  2.0  2.0\n",
       " 2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0     2.0  2.0  2.0  2.0  2.0  2.0  2.0\n",
       " 2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0     2.0  2.0  2.0  2.0  2.0  2.0  2.0\n",
       " 2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0     2.0  2.0  2.0  2.0  2.0  2.0  2.0\n",
       " 2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0     2.0  2.0  2.0  2.0  2.0  2.0  2.0\n",
       " 2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0  …  2.0  2.0  2.0  2.0  2.0  2.0  2.0\n",
       " 2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0     2.0  2.0  2.0  2.0  2.0  2.0  2.0\n",
       " 2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0     2.0  2.0  2.0  2.0  2.0  2.0  2.0\n",
       " 2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0     2.0  2.0  2.0  2.0  2.0  2.0  2.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "B = CUDA.ones(1024,1024)\n",
    "kernel2 = mul2(CUDADevice(),16)\n",
    "event = kernel2(B,ndrange=size(B))\n",
    "wait(event)\n",
    "@show B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lenguaje de kernel\n",
    "\n",
    "En una función que contenga la macro `@kernel` se puede utilizar el lenguaje de kernel incluíso en el paquete:\n",
    "\n",
    "* `@Const`: Permite declarar un argumento que es constante. Es decir, no se escribe como parte del kernel ni hace referencia a otra memoria en el kernel.\n",
    "\n",
    "* `@index`: Permite obtener el índice del entorno dentro de un kernel. Puedes obtener tanto un índice lineal como uno cartesiano.\n",
    "    ### Granularidad del Índice\n",
    "    * `Global`: Usado para acceder a memoria global.\n",
    "    * `Group`: El índice del grupo de trabajo.\n",
    "    * `Local`: El índice dentro del grupo de trabajo.\n",
    "    ### Tipo de Índice\n",
    "    * `Linear`: Produce un `Int64` que se usa para acceder de manera lineal a la memoria.\n",
    "    * `Cartesian`: Produce un `CartesianIndex{N}` para acceder a la memoria.\n",
    "    * `NTuple`: Produce un `NTuple{N}` para acceder a la memoria.\n",
    "El argumento or defecto es `Linear`.\n",
    "\n",
    "```Julia\n",
    "    @index(Global, Linear)\n",
    "    @index(Global, Cartesian)\n",
    "    @index(Local, Cartesian)\n",
    "    @index(Group, Linear)\n",
    "    @index(Local, NTuple)\n",
    "    @index(Global)\n",
    "```\n",
    "\n",
    "* `@localmem`: Declara una memoria local para un grupo de trabajo.\n",
    "\n",
    "* `@private`: Declara memoria que es local para cada elemento en el grupo de trabajo. Se puede usar de manera segura para sincronizar los hilos.\n",
    "\n",
    "* `@synchronize`: Después de esta sentencia todas las lecturas y escrituras a memoria local y global de cada hilo en el grupo de trabajo.\n",
    "\n",
    "* `@print`: Sirve para imprimir dentro del kernel.\n",
    "\n",
    "## Ejemplos:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "copy_kernel! (generic function with 5 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@kernel function copy_kernel!(A, @Const B)\n",
    "    I = @index(Global)\n",
    "    @inbounds A[I] = B[I]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mycopy! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function mycopy!(A::Array, B::Array)\n",
    "    @assert size(A) == size(B)\n",
    "    kernel = copy_kernel!(CPU(), 8)\n",
    "    kernel(A,B, ndrange=length(A))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m\n",
       "  Expression: A == B\n",
       "   Evaluated: [1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; … ; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0] == [1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; … ; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A = zeros(128,128)\n",
    "B = ones(128,128)\n",
    "event = mycopy!(A,B)\n",
    "wait(event)\n",
    "@test A == B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m\n",
       "  Expression: A == B\n",
       "   Evaluated: Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0] == Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if has_cuda() && has_cuda_gpu()\n",
    "    function mycopy!(A::CuArray, B::CuArray)\n",
    "        @assert size(A) == size(B)\n",
    "        copy_kernel!(CUDADevice(), 256)(A, B, ndrange=length(A))\n",
    "    end\n",
    "\n",
    "    A = CuArray{Float32}(undef, 1024)\n",
    "    B = CUDA.ones(Float32, 1024)\n",
    "    event = mycopy!(A,B)\n",
    "    wait(event)\n",
    "    @test A == B\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparación entre CUDA.jl y KernelAbstractions.jl\n",
    "\n",
    "En general, uno puede usar ambas librerías para construir kernels para GPU; sin embargo, los kernels construidos con `CUDA.jl` no pueden ejecutarse con el CPU, mientras que con `KernelAbstractions.jl` sí es posible. Los tipos de datos que usan ambos son los mismos (`CuArray`), a excepción de la manera de declarar memoria compartida. Para mostrar la diferencia de uso de ambos paquetes, crearemos el mismo programa usando ambas bibliotecas. El programa tiene como objetivo obtener la matriz transpuesta de una matriz, primero realizado de manera \"ingenua\", y luego usando memoria compartida para mejorar el uso de memoria y ancho de banda del GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matriz Transpuesta Ingenua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transpose_kernel_nv! (generic function with 5 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using KernelAbstractions, CUDA, CUDAKernels\n",
    "\n",
    "@kernel function transpose_kernel_nv!(A, @Const B)\n",
    "    I,J = @index(Global, NTuple)\n",
    "    @inbounds A[I,J] = B[J,I]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64×64 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:\n",
       " 0.974954   0.0790626  0.46719    …  0.22633    0.689319  0.635561\n",
       " 0.0561513  0.221508   0.826807      0.170608   0.632967  0.864559\n",
       " 0.752618   0.572047   0.984443      0.268353   0.414733  0.812482\n",
       " 0.838736   0.870736   0.869567      0.483663   0.525199  0.904956\n",
       " 0.830334   0.367194   0.584099      0.866764   0.873547  0.384195\n",
       " 0.395576   0.976292   0.0638496  …  0.0383313  0.435954  0.804388\n",
       " 0.310672   0.765672   0.306926      0.0805827  0.94806   0.597387\n",
       " 0.756049   0.0590058  0.425904      0.819202   0.665524  0.578942\n",
       " 0.679445   0.723373   0.0528807     0.188758   0.982613  0.777483\n",
       " 0.593358   0.292923   0.964337      0.0864089  0.947523  0.144051\n",
       " ⋮                                ⋱                       \n",
       " 0.679124   0.380334   0.383251   …  0.498911   0.890486  0.0675879\n",
       " 0.739486   0.961227   0.104527      0.564903   0.348581  0.843546\n",
       " 0.338623   0.275283   0.642112      0.491931   0.275179  0.422221\n",
       " 0.743985   0.332102   0.129979      0.661344   0.3297    0.026663\n",
       " 0.197019   0.373242   0.29444       0.82842    0.914274  0.917669\n",
       " 0.84042    0.549415   0.34879    …  0.799116   0.399096  0.93003\n",
       " 0.0250071  0.348352   0.852856      0.705175   0.138585  0.765451\n",
       " 0.994878   0.357737   0.228906      0.322572   0.721268  0.362612\n",
       " 0.83434    0.976315   0.930973      0.350817   0.558942  0.0505435"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "B  = CUDA.rand(64,64)\n",
    "A  = similar(B)\n",
    "Ax = similar(B)\n",
    "\n",
    "const transpose! = transpose_kernel_nv!(CUDADevice(), (32,32))\n",
    "event = transpose!(A,B, ndrange=size(A))\n",
    "wait(event)\n",
    "@show A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64×64 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:\n",
       " 0.996059    0.91271   0.669744   …  0.590227   0.72542    0.360778\n",
       " 0.802734    0.761051  0.516635      0.11815    0.796344   0.388488\n",
       " 0.0012281   0.668878  0.156632      0.698013   0.576733   0.59419\n",
       " 0.559936    0.351366  0.0664896     0.961207   0.494534   0.123446\n",
       " 0.00266297  0.350677  0.0445517     0.705549   0.112994   0.309934\n",
       " 0.352857    0.938405  0.644025   …  0.329602   0.0928931  0.0380282\n",
       " 0.240857    0.803277  0.476764      0.781645   0.452563   0.049848\n",
       " 0.341734    0.854166  0.0367136     0.427318   0.681104   0.800245\n",
       " 0.0156344   0.151619  0.402855      0.168077   0.654153   0.64049\n",
       " 0.467154    0.669903  0.731971      0.544794   0.465232   0.651381\n",
       " ⋮                                ⋱                        \n",
       " 0.526856    0.95149   0.17509    …  0.788622   0.864167   0.762459\n",
       " 0.210794    0.478757  0.892773      0.582176   0.343771   0.776082\n",
       " 0.315545    0.97205   0.264825      0.983838   0.0130996  0.124601\n",
       " 0.609475    0.81133   0.680416      0.351897   0.232582   0.601458\n",
       " 0.463406    0.516233  0.72626       0.0968994  0.939731   0.299694\n",
       " 0.168226    0.311639  0.369968   …  0.779929   0.980536   0.653118\n",
       " 0.173762    0.984321  0.367999      0.737099   0.020311   0.621755\n",
       " 0.0855956   0.677621  0.906635      0.93295    0.252077   0.417916\n",
       " 0.586098    0.973494  0.35023       0.951456   0.470843   0.540032"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@show B "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matriz_transpose_cd_nv! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function matriz_transpose_cd_nv!(A, B)\n",
    "    indexX = threadIdx().x + (blockIdx().x - 1) * blockDim().x \n",
    "    indexY = threadIdx().y + (blockIdx().y - 1) * blockDim().y\n",
    "    \n",
    "    for j in 0:blockDim().y:blockDim().x-1\n",
    "        A[indexX,indexY+j] = B[indexY + j, indexX]\n",
    "    end\n",
    "\n",
    "    return nothing\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CUDA.HostKernel{typeof(matriz_transpose_cd_nv!), Tuple{CuDeviceMatrix{Float32, 1}, CuDeviceMatrix{Float32, 1}}}(matriz_transpose_cd_nv!, CuFunction(Ptr{Nothing} @0x00000000be4551b0, CuModule(Ptr{Nothing} @0x00000000be013aa0, CuContext(0x0000000067b3f7d0, instance 6b12de4ee8104e7d))), CUDA.KernelState(Ptr{Nothing} @0x0000000603e00000))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dimGrid  = (Int(64/32),Int(64/32),1)\n",
    "dimBlock = (32,8,1)\n",
    "\n",
    "@cuda threads=dimBlock  blocks=dimGrid matriz_transpose_cd_nv!(Ax,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64×64 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " ⋮                        ⋮              ⋱                 ⋮              \n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@show Ax"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
