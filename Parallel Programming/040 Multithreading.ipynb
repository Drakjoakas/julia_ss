{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m     Cloning\u001b[22m\u001b[39m git-repo `https://github.com/JuliaComputing/JuliaAcademyData.jl`\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaComputing/JuliaAcademyData.jl`\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m registry at `C:\\Users\\kart-\\.julia\\registries\\General`\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaRegistries/General.git`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `C:\\Users\\kart-\\.julia\\environments\\v1.6\\Project.toml`\n",
      " \u001b[90m [18b7da76] \u001b[39m\u001b[92m+ JuliaAcademyData v0.1.0 `https://github.com/JuliaComputing/JuliaAcademyData.jl#main`\u001b[39m\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `C:\\Users\\kart-\\.julia\\environments\\v1.6\\Manifest.toml`\n",
      " \u001b[90m [18b7da76] \u001b[39m\u001b[92m+ JuliaAcademyData v0.1.0 `https://github.com/JuliaComputing/JuliaAcademyData.jl#main`\u001b[39m\n",
      "\u001b[32m\u001b[1mPrecompiling\u001b[22m\u001b[39m project...\n",
      "\u001b[32m  ✓ \u001b[39mJuliaAcademyData\n",
      "  1 dependency successfully precompiled in 3 seconds (92 already precompiled)\n",
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m environment at `C:\\Users\\kart-\\.julia\\packages\\JuliaAcademyData\\3C2GY\\courses\\Parallel_Computing\\Project.toml`\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m ArrayInterface ───────── v2.2.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Reexport ─────────────── v0.2.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m QuadGK ───────────────── v2.3.1\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m BinDeps ──────────────── v1.0.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m BinaryProvider ───────── v0.5.8\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m StatsBase ────────────── v0.32.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m CUDAdrv ──────────────── v5.0.1\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Gadfly ───────────────── v1.0.1\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m SortingAlgorithms ────── v0.3.1\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m FixedPointNumbers ────── v0.6.1\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Conda ────────────────── v1.3.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Compat ───────────────── v2.2.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m SpecialFunctions ─────── v0.8.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m CommonSubexpressions ─── v0.2.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m AxisAlgorithms ───────── v1.0.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m ForwardDiff ──────────── v0.10.9\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Zygote ───────────────── v0.4.6\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m DocStringExtensions ──── v0.8.1\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Parameters ───────────── v0.12.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m DataAPI ──────────────── v1.1.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m KernelDensity ────────── v0.5.1\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m FillArrays ───────────── v0.8.4\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m StatsFuns ────────────── v0.9.3\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Flux ─────────────────── v0.10.1\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m CuArrays ─────────────── v1.6.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m VersionParsing ───────── v1.2.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m AbstractFFTs ─────────── v0.5.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Arpack ───────────────── v0.3.2\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Hwloc ────────────────── v1.0.3\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Rmath ────────────────── v0.6.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m ColorTypes ───────────── v0.8.1\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Distances ────────────── v0.8.2\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m WoodburyMatrices ─────── v0.5.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Contour ──────────────── v0.5.1\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m PositiveFactorizations ─ v0.2.3\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m NLSolversBase ────────── v7.5.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m IndirectArrays ───────── v0.5.1\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Showoff ──────────────── v0.3.1\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Parsers ──────────────── v0.3.10\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m JSON ─────────────────── v0.21.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m CodecZlib ────────────── v0.6.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m StaticArrays ─────────── v0.12.1\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Missings ─────────────── v0.4.3\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Colors ───────────────── v0.9.6\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Interpolations ───────── v0.12.7\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m DistributedArrays ────── v0.6.4\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m TimerOutputs ─────────── v0.5.3\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Juno ─────────────────── v0.7.2\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m DiffRules ────────────── v1.0.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m ZygoteRules ──────────── v0.2.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m OrderedCollections ───── v1.1.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m CoupledFields ────────── v0.1.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m CUDAnative ───────────── v2.7.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Ratios ───────────────── v0.3.1\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m ZipFile ──────────────── v0.8.4\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m PDMats ───────────────── v0.9.11\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Primes ───────────────── v0.4.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m LineSearches ─────────── v7.0.1\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m NNlib ────────────────── v0.6.4\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m CEnum ────────────────── v0.2.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m NaNMath ──────────────── v0.3.3\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m OffsetArrays ─────────── v1.0.2\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m CategoricalArrays ────── v0.7.7\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m TranscodingStreams ───── v0.9.5\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Adapt ────────────────── v1.0.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m GPUArrays ────────────── v2.0.1\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m DiffEqDiffTools ──────── v1.6.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Requires ─────────────── v0.5.2\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m IterTools ────────────── v1.3.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Optim ────────────────── v0.20.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m URIParser ────────────── v0.4.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m CUDAapi ──────────────── v2.1.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m DiffResults ──────────── v1.0.2\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m BenchmarkTools ───────── v0.4.3\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m IRTools ──────────────── v0.3.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m FFTW ─────────────────── v1.1.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Compose ──────────────── v0.8.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m DataStructures ───────── v0.17.9\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Distributions ────────── v0.22.3\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m LLVM ─────────────────── v1.3.3\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m AbstractTrees ────────── v0.3.1\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Loess ────────────────── v0.5.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m MacroTools ───────────── v0.5.3\n",
      "\u001b[32m\u001b[1m    Building\u001b[22m\u001b[39m SpecialFunctions → `C:\\Users\\kart-\\.julia\\scratchspaces\\44cfe95a-1eb2-52ea-b672-e2afdf69b78f\\3bdd374b6fd78faf0119b8c5d538788dbf910c6e\\build.log`\n",
      "\u001b[32m\u001b[1m    Building\u001b[22m\u001b[39m Conda ───────────→ `C:\\Users\\kart-\\.julia\\scratchspaces\\44cfe95a-1eb2-52ea-b672-e2afdf69b78f\\9a11d428dcdc425072af4aea19ab1e8c3e01c032\\build.log`\n",
      "\u001b[32m\u001b[1m    Building\u001b[22m\u001b[39m Arpack ──────────→ `C:\\Users\\kart-\\.julia\\scratchspaces\\44cfe95a-1eb2-52ea-b672-e2afdf69b78f\\cd64c112638582ba4f0be9c3e20656499c508565\\build.log`\n",
      "\u001b[32m\u001b[1m    Building\u001b[22m\u001b[39m Hwloc ───────────→ `C:\\Users\\kart-\\.julia\\scratchspaces\\44cfe95a-1eb2-52ea-b672-e2afdf69b78f\\bb23d264d76b82d1da80733cbb01bad8a11ae489\\build.log`\n",
      "\u001b[32m\u001b[1m    Building\u001b[22m\u001b[39m Rmath ───────────→ `C:\\Users\\kart-\\.julia\\scratchspaces\\44cfe95a-1eb2-52ea-b672-e2afdf69b78f\\2bbddcb984a1d08612d0c4abb5b4774883f6fa98\\build.log`\n",
      "\u001b[32m\u001b[1m    Building\u001b[22m\u001b[39m CodecZlib ───────→ `C:\\Users\\kart-\\.julia\\scratchspaces\\44cfe95a-1eb2-52ea-b672-e2afdf69b78f\\05916673a2627dd91b4969ff8ba6941bc85a960e\\build.log`\n",
      "\u001b[32m\u001b[1m    Building\u001b[22m\u001b[39m ZipFile ─────────→ `C:\\Users\\kart-\\.julia\\scratchspaces\\44cfe95a-1eb2-52ea-b672-e2afdf69b78f\\7fbfbc51c186f0ccdbe091f32d3dff8608973f8e\\build.log`\n",
      "\u001b[32m\u001b[1m    Building\u001b[22m\u001b[39m NNlib ───────────→ `C:\\Users\\kart-\\.julia\\scratchspaces\\44cfe95a-1eb2-52ea-b672-e2afdf69b78f\\755c0bab3912ff782167e1b4b774b833f8a0e550\\build.log`\n",
      "\u001b[32m\u001b[1m    Building\u001b[22m\u001b[39m FFTW ────────────→ `C:\\Users\\kart-\\.julia\\scratchspaces\\44cfe95a-1eb2-52ea-b672-e2afdf69b78f\\4cfd3d43819228b9e73ab46600d0af0aa5cedceb\\build.log`\n",
      "\u001b[32m\u001b[1mPrecompiling\u001b[22m\u001b[39m project...\n",
      "\u001b[33m  ✓ \u001b[39m\u001b[90mVersionParsing\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mNaNMath\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mZipFile\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mTimerOutputs\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mRmath\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39mFillArrays\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mFixedPointNumbers\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mDataAPI\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mIndirectArrays\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mAbstractFFTs\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mAbstractTrees\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mOffsetArrays\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mShowoff\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mDistances\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mOrderedCollections\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mCUDAapi\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mArpack\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mRequires\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mURIParser\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mCEnum\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mPositiveFactorizations\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mIterTools\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39mHwloc\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mAdapt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mCommonSubexpressions\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mDocStringExtensions\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mWoodburyMatrices\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mReexport\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mTranscodingStreams\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mPrimes\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMissings\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mCompat\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mLoess\u001b[39m\n",
      "\u001b[33m  ✓ \u001b[39m\u001b[90mParsers\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mParameters\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mArrayInterface\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mPDMats\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mNNlib\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mDataStructures\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mBinDeps\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mStaticArrays\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mCodecZlib\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mAxisAlgorithms\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mRatios\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mGPUArrays\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mSortingAlgorithms\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39mDistributedArrays\n",
      "\u001b[33m  ✓ \u001b[39m\u001b[90mJSON\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mContour\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mQuadGK\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mColorTypes\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mDiffResults\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mDiffEqDiffTools\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMacroTools\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mSpecialFunctions\u001b[39m\n",
      "\u001b[33m  ✓ \u001b[39m\u001b[90mConda\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39mBenchmarkTools\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mLLVM\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mInterpolations\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mStatsBase\u001b[39m\n",
      "\u001b[91m  ✗ \u001b[39m\u001b[90mCUDAdrv\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mDiffRules\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMedia\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39mColors\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mZygoteRules\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mCoupledFields\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mFFTW\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mIRTools\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mStatsFuns\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mJuno\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mCategoricalArrays\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mForwardDiff\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39mCompose\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mNLSolversBase\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mDistributions\u001b[39m\n",
      "\u001b[91m  ✗ \u001b[39m\u001b[90mZygote\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mLineSearches\u001b[39m\n",
      "\u001b[91m  ✗ \u001b[39m\u001b[90mCUDAnative\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mOptim\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mKernelDensity\u001b[39m\n",
      "\u001b[91m  ✗ \u001b[39mCuArrays\n",
      "\u001b[91m  ✗ \u001b[39mFlux\n",
      "\u001b[32m  ✓ \u001b[39mGadfly\n",
      "  78 dependencies successfully precompiled in 28 seconds (4 already precompiled)\n",
      "  \u001b[33m4\u001b[39m dependencies precompiled but different versions are currently loaded. Restart julia to access the new versions\n",
      "  \u001b[91m5\u001b[39m dependencies errored. To see a full report either run `import Pkg; Pkg.precompile()` or load the packages\n"
     ]
    }
   ],
   "source": [
    "import Pkg; Pkg.add(Pkg.PackageSpec(url=\"https://github.com/JuliaComputing/JuliaAcademyData.jl\"))\n",
    "using JuliaAcademyData; activate(\"Parallel_Computing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multithreading\n",
    "\n",
    "Now we're finally ready to start talking about running things on multiple\n",
    "processors! Most computers (even cell phones) these days have multiple cores\n",
    "or processors — so the obvious place to start working with parallelism is\n",
    "making use of those from within our Julia process.\n",
    "\n",
    "The first challenge, though, is knowing precisely how many \"processors\" you have.\n",
    "\"Processors\" is in scare quotes because, well, it's complicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "versioninfo(verbose = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ;cat /proc/cpuinfo # on Linux machines\n",
    "\n",
    "using Hwloc\n",
    "Hwloc.num_physical_cores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What your computer reports as the number of processors might not be the same\n",
    "as the total number of \"cores\". While sometimes virtual processors can add\n",
    "performance, parallelizing a typical numerical computation over these virtual\n",
    "processors will lead to significantly worse performance because they still\n",
    "have to share much of the nuts and bolts of the computation hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Julia is somewhat multithreaded by default! BLAS calls (like matrix multiplication) are\n",
    "already threaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling BenchmarkTools [6e4b80f9-dd63-53aa-95a3-0cdb28fa8baf]\n",
      "└ @ Base loading.jl:1342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  81.959 ms (2 allocations: 30.52 MiB)\n"
     ]
    }
   ],
   "source": [
    "using BenchmarkTools\n",
    "A = rand(2000, 2000);\n",
    "B = rand(2000, 2000);\n",
    "@btime $A*$B;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is — by default — already using all your CPU cores! You can see the effect\n",
    "by changing the number of threads (which BLAS supports doing dynamically):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  281.080 ms (2 allocations: 30.52 MiB)\n",
      "  103.533 ms (2 allocations: 30.52 MiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2000×2000 Matrix{Float64}:\n",
       " 506.782  502.772  505.263  498.195  …  504.752  490.158  493.711  495.897\n",
       " 519.701  511.185  525.351  518.236     524.026  510.359  508.171  514.803\n",
       " 500.737  493.335  491.528  496.667     496.745  487.804  488.734  486.753\n",
       " 508.342  503.622  519.13   501.521     508.156  496.494  491.871  497.413\n",
       " 506.386  499.859  504.492  493.573     499.404  491.191  491.92   497.941\n",
       " 506.547  498.667  510.92   502.11   …  504.466  492.785  493.748  495.487\n",
       " 526.076  522.079  521.487  514.272     517.46   507.725  503.569  510.609\n",
       " 506.637  510.664  517.562  505.809     504.578  500.122  497.75   497.964\n",
       " 500.934  501.14   506.628  497.119     500.818  491.313  495.665  499.44\n",
       " 495.333  482.678  494.667  488.883     490.834  478.53   487.437  493.619\n",
       " 500.238  494.678  503.295  494.821  …  502.669  491.462  489.396  499.157\n",
       " 501.896  497.607  509.867  496.564     502.774  492.011  484.884  488.658\n",
       " 508.102  509.826  509.513  510.778     511.124  494.957  501.623  500.455\n",
       "   ⋮                                 ⋱                             \n",
       " 511.754  509.186  516.91   509.303     511.359  496.881  504.074  501.983\n",
       " 496.672  501.059  505.943  493.917     493.973  487.589  478.866  485.193\n",
       " 498.179  496.544  512.124  496.952  …  508.829  487.616  489.514  492.486\n",
       " 501.301  503.042  512.726  508.113     505.326  494.685  498.383  495.883\n",
       " 502.852  506.16   513.002  494.034     502.53   487.765  490.624  497.96\n",
       " 510.066  509.327  509.792  497.821     514.863  498.232  497.532  502.636\n",
       " 506.066  510.941  513.729  504.267     508.305  495.755  498.63   495.375\n",
       " 510.933  509.563  505.172  513.535  …  513.154  491.512  498.505  505.195\n",
       " 504.993  502.416  511.567  495.728     508.187  494.039  500.758  492.354\n",
       " 498.772  495.136  504.269  493.711     503.476  490.808  491.647  497.645\n",
       " 506.779  503.292  518.353  502.783     509.852  500.727  498.927  502.014\n",
       " 504.478  495.19   507.721  505.385     499.802  492.497  493.549  493.974"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "BLAS.set_num_threads(1)\n",
    "@btime $A*$B\n",
    "BLAS.set_num_threads(4)\n",
    "@btime $A*$B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does it look like to implement your _own_ threaded algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multithreading support is marked as \"experimental\" for Julia 1.0 and is\n",
    "pending a big revamp for Julia version 1.2 or 1.3. The core tenets will be\n",
    "the same, but it should become much easier to use efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using .Threads\n",
    "\n",
    "nthreads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Julia currently needs to start up knowing that it has threading support enabled.\n",
    "\n",
    "You do that with a environment variable. To get four threads, start Julia with:\n",
    "\n",
    "```\n",
    "JULIA_NUM_THREADS=4 julia\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On JuliaBox, this is a challenge — we don't have access to the launching process!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ";env JULIA_NUM_THREADS=4 julia -E 'using .Threads; nthreads()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using .Threads\n",
    "Threads.nthreads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threadid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we're currently on thread 1. Of course a loop like this will\n",
    "just set the first element to one a number of times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Union{Missing, Int64}}:\n",
       " 1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = Array{Union{Int,Missing}}(missing, nthreads())\n",
    "for i in 1:nthreads()\n",
    "    A[threadid()] = threadid()\n",
    "end\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we prefix it with `@threads` then the loop body runs on all threads!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Union{Missing, Int64}}:\n",
       " 1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@threads for i in 1:nthreads()\n",
    "    A[threadid()] = threadid()\n",
    "end\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's try implementing our first simple threaded algorithm — `sum`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.248875 seconds (20.00 M allocations: 305.176 MiB, 6.79% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.999846923532342e6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function threaded_sum1(A)\n",
    "    r = zero(eltype(A))\n",
    "    @threads for i in eachindex(A)\n",
    "        @inbounds r += A[i]\n",
    "    end\n",
    "    return r\n",
    "end\n",
    "\n",
    "A = rand(10_000_000)\n",
    "threaded_sum1(A)\n",
    "@time threaded_sum1(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.004362 seconds (1 allocation: 16 bytes)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.999846923532457e6"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(A)\n",
    "@time sum(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoa! What happened? Not only did we get the wrong answer, it was _slow_ to get it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.070153 seconds (9 allocations: 976 bytes)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.999846923532342e6"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function threaded_sum2(A)\n",
    "    r = Atomic{eltype(A)}(zero(eltype(A)))\n",
    "    @threads for i in eachindex(A)\n",
    "        @inbounds atomic_add!(r, A[i])\n",
    "    end\n",
    "    return r[]\n",
    "end\n",
    "\n",
    "threaded_sum2(A)\n",
    "@time threaded_sum2(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright! Now we got the correct answer (modulo some floating point associativity),\n",
    "but it's still slower than just doing the simple thing on 1 core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threaded_sum2(A) ≈ sum(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it's still slow! Using atomics is much slower than just adding integers\n",
    "because we constantly have to go and check _which_ processor has the latest\n",
    "work! Also remember that each thread is running on its own processor — and\n",
    "that processor also supports SIMD!  Well, that is if it didn't need to worry\n",
    "about syncing up with the other processors..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.004077 seconds (8 allocations: 656 bytes)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.999846923532452e6"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function threaded_sum3(A)\n",
    "    r = Atomic{eltype(A)}(zero(eltype(A)))\n",
    "    len, rem = divrem(length(A), nthreads())\n",
    "    @threads for t in 1:nthreads()\n",
    "        rₜ = zero(eltype(A))\n",
    "        @simd for i in (1:len) .+ (t-1)*len\n",
    "            @inbounds rₜ += A[i]\n",
    "        end\n",
    "        atomic_add!(r, rₜ)\n",
    "    end\n",
    "    # catch up any stragglers\n",
    "    result = r[]\n",
    "    @simd for i in length(A)-rem+1:length(A)\n",
    "        @inbounds result += A[i]\n",
    "    end\n",
    "    return result\n",
    "end\n",
    "\n",
    "threaded_sum3(A)\n",
    "@time threaded_sum3(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dang, that's complicated. There's also a problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "TypeError: in Atomic, in T, expected T<:Union{Bool, Float16, Float32, Float64, Int128, Int16, Int32, Int64, Int8, UInt128, UInt16, UInt32, UInt64, UInt8}, got Type{ComplexF64}",
     "output_type": "error",
     "traceback": [
      "TypeError: in Atomic, in T, expected T<:Union{Bool, Float16, Float32, Float64, Int128, Int16, Int32, Int64, Int8, UInt128, UInt16, UInt32, UInt64, UInt8}, got Type{ComplexF64}",
      "",
      "Stacktrace:",
      " [1] threaded_sum3(A::Vector{ComplexF64})",
      "   @ Main .\\In[13]:2",
      " [2] top-level scope",
      "   @ In[14]:1",
      " [3] eval",
      "   @ .\\boot.jl:360 [inlined]",
      " [4] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base .\\loading.jl:1116"
     ]
    }
   ],
   "source": [
    "threaded_sum3(rand(10) .+ rand(10)im) # try an array of complex numbers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isn't there an easier way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = zeros(eltype(A), nthreads())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function threaded_sum4(A)\n",
    "    R = zeros(eltype(A), nthreads())\n",
    "    @threads for i in eachindex(A)\n",
    "        @inbounds R[threadid()] += A[i]\n",
    "    end\n",
    "    r = zero(eltype(A))\n",
    "    # sum the partial results from each thread\n",
    "    for i in eachindex(R)\n",
    "        @inbounds r += R[i]\n",
    "    end\n",
    "    return r\n",
    "end\n",
    "\n",
    "threaded_sum4(A)\n",
    "@time threaded_sum4(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sacrifices our ability to `@simd` so it's a little slower, but at least we don't need to worry\n",
    "about all those indices! And we also don't need to worry about atomics and\n",
    "can again support arrays of any elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threaded_sum4(rand(10) .+ rand(10)im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key takeaways from `threaded_sum`:\n",
    "\n",
    "* Beware shared state across threads — it may lead to wrong answers!\n",
    "    * Protect yourself by using atomics (or [locks/mutexes](https://docs.julialang.org/en/v1/base/multi-threading/#Synchronization-Primitives-1))\n",
    "    * Better yet: divide up the work manually such that the inner loops don't\n",
    "      share state. `@threads for i in 1:nthreads()` is a handy idiom.\n",
    "    * Alternatively, just use an array and only access a single thread's elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beware of global state (even if it's not obvious!)\n",
    "\n",
    "Another class of algorithm that you may want to parallelize is a monte-carlo\n",
    "problem. Since each iteration is a new random draw, and since you're interested\n",
    "in looking at the aggregate result, this seems like it should lend itself to\n",
    "parallelism quite nicely!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function serialpi(n)\n",
    "    inside = 0\n",
    "    for i in 1:n\n",
    "        x, y = rand(), rand()\n",
    "        inside += (x^2 + y^2 <= 1)\n",
    "    end\n",
    "    return 4 * inside / n\n",
    "end\n",
    "serialpi(1)\n",
    "@time serialpi(100_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using .Threads\n",
    "nthreads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the techniques we learned above to make a fast threaded implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function threadedpi(n)\n",
    "    inside = zeros(Int, nthreads())\n",
    "    @threads for i in 1:n\n",
    "        x, y = rand(), rand()\n",
    "        @inbounds inside[threadid()] += (x^2 + y^2 <= 1)\n",
    "    end\n",
    "    return 4 * sum(inside) / n\n",
    "end\n",
    "threadedpi(100_000_000)\n",
    "@time threadedpi(100_000_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now why didn't that work?  It's slow! Let's look at the sequence of random\n",
    "numbers that we generate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Random\n",
    "Random.seed!(0)\n",
    "N = 20000\n",
    "Rserial = zeros(N)\n",
    "for i in 1:N\n",
    "    Rserial[i] = rand()\n",
    "end\n",
    "Rserial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(0)\n",
    "Rthreaded = zeros(N)\n",
    "@threads for i in 1:N\n",
    "    Rthreaded[i] = rand()\n",
    "end\n",
    "Rthreaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Set(Rserial) == Set(Rthreaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexin(Rserial, Rthreaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aha, `rand()` isn't threadsafe! It's mutating (and reading) some global each\n",
    "time to figure out what to get next. This leads to slowdowns — and worse — it\n",
    "skews the generated distribution of random numbers since some are repeated!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const ThreadRNG = Vector{Random.MersenneTwister}(undef, nthreads())\n",
    "@threads for i in 1:nthreads()\n",
    "    ThreadRNG[Threads.threadid()] = Random.MersenneTwister()\n",
    "end\n",
    "function threadedpi2(n)\n",
    "    inside = zeros(Int, nthreads())\n",
    "    len, rem = divrem(n, nthreads())\n",
    "    rem == 0 || error(\"use a multiple of $(nthreads()), please!\")\n",
    "    @threads for i in 1:nthreads()\n",
    "        rng = ThreadRNG[threadid()]\n",
    "        v = 0\n",
    "        for j in 1:len\n",
    "            x, y = rand(rng), rand(rng)\n",
    "            v += (x^2 + y^2 <= 1)\n",
    "        end\n",
    "        inside[threadid()] = v\n",
    "    end\n",
    "    return 4 * sum(inside) / n\n",
    "end\n",
    "threadedpi2(10)\n",
    "@time threadedpi2(100_000_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an aside, be careful about initializing many `MersenneTwister`s with\n",
    "different states. Better to use [`randjump`](https://docs.julialang.org/en/v1/manual/parallel-computing/#Side-effects-and-mutable-function-arguments-1) to skip ahead for a single state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beware oversubscription\n",
    "\n",
    "Remember how BLAS is threaded by default? What happens if we try to `@threads`\n",
    "something that uses BLAS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ms = [rand(1000, 1000) for _ in 1:100]\n",
    "function serial_matmul(As)\n",
    "    first_idxs = zeros(length(As))\n",
    "    for i in eachindex(As)\n",
    "        @inbounds first_idxs[i] = (As[i]'*As[i])[1]\n",
    "    end\n",
    "    first_idxs\n",
    "end\n",
    "serial_matmul(Ms);\n",
    "@time serial_matmul(Ms);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "BLAS.set_num_threads(nthreads()) # Explicitly tell BLAS to use the same number of threads\n",
    "function threaded_matmul(As)\n",
    "    first_idxs = zeros(length(As))\n",
    "    @threads for i in eachindex(As)\n",
    "        @inbounds first_idxs[i] = (As[i]'*As[i])[1]\n",
    "    end\n",
    "    first_idxs\n",
    "end\n",
    "threaded_matmul(Ms)\n",
    "@time threaded_matmul(Ms);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLAS.set_num_threads(1)\n",
    "@time threaded_matmul(Ms);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time serial_matmul(Ms) # Again, now that BLAS has just 1 thread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beware \"false sharing\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the memory latency table?\n",
    "\n",
    "\n",
    "| System Event                   | Actual Latency | Scaled Latency |                          |\n",
    "| ------------------------------ | -------------- | -------------- | ------------------------ |\n",
    "| One CPU cycle                  |     0.4 ns     |     1 s        | ← work happens here     |\n",
    "| Level 1 cache access           |     0.9 ns     |     2 s        |                          |\n",
    "| Level 2 cache access           |     2.8 ns     |     7 s        |                          |\n",
    "| Level 3 cache access           |      28 ns     |     1 min      |                          |\n",
    "| Main memory access (DDR DIMM)  |    ~100 ns     |     4 min      | ← we have control here  |\n",
    "\n",
    "This is what a typical modern cpu looks like:\n",
    "\n",
    "![Intel Core i7](https://raw.githubusercontent.com/JuliaComputing/JuliaAcademyData.jl/master/courses/Parallel_Computing/images/i7.jpg)\n",
    "\n",
    "Multiple cores on the same processor share the L3 cache, but do not share L1 and L2 caches! So what happens if we're accessing and mutating data from the same array across multiple cores?\n",
    "\n",
    "![Cache coherency](https://raw.githubusercontent.com/JuliaComputing/JuliaAcademyData.jl/master/courses/Parallel_Computing/images/false-sharing.gif)\n",
    "\n",
    "Unlike \"true\" sharing — which we saw above — false sharing will still return the correct answer! But it does so at the cost of performance. The cores recognize they don't have exclusive access to the cache line and so upon modification they alert all other cores to invalidate and re-fetch the data.\n",
    "\n",
    "```julia\n",
    "function threaded_sum4(A)\n",
    "    R = zeros(eltype(A), nthreads())\n",
    "    @threads for i in eachindex(A)\n",
    "        @inbounds R[threadid()] += A[i]\n",
    "    end\n",
    "    r = zero(eltype(A))\n",
    "    # sum the partial results from each thread\n",
    "    for i in eachindex(R)\n",
    "        @inbounds r += R[i]\n",
    "    end\n",
    "    return r\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further improvements coming here!\n",
    "\n",
    "PARTR — the threading improvement I discussed at the beginning aims to address\n",
    "this problem of having library functions implemented with `@threads` and then\n",
    "having callers call them with `@threads`. Uses a state-of-the-art work queue\n",
    "mechanism to make sure that all threads stay busy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threading takeaways:\n",
    "\n",
    "* It's easy! Just start Julia with `JULIA_NUM_THREADS` and tack a `@threads` on your loop\n",
    "* Well, not so fast\n",
    "    * Be aware of your hardware to set `JULIA_NUM_THREADS` appropiately\n",
    "    * Beware shared state (for both performance and correctness)\n",
    "    * Beware global state (even if it's not obvious)\n",
    "    * Beware false sharing (if Julia/LLVM don't handle it for you)\n",
    "* We need to think carefully about how to design parallel algorithms!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.5",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 3
}
